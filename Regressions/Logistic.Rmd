---
title: "Logistic regression"
author: "P-value Data Analytics"
date: "4/13/2021"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Goal - Assessing the multivariante correlation between ltv (0 or no 0) and multiple variables  

# Libraries
```{r, warning=FALSE, message=FALSE}
pacman::p_load(tidyverse, skimr, rcompanion, ResourceSelection)
```


# Uploading a data set from the Git Hub

1) Go to the GitHub repository link where you have the CSV file.
2) Click on the **raw** option present on the top right of the data.
3) This will open a new window in the browser.
4) The link should be like **https://raw.githubusercontent.com/..**.
5) You have to use this link to download csv file from Github.

```{r, warning=FALSE, message=FALSE}
URL <- c('https://raw.githubusercontent.com/tomushkat/R-course-for-data-management-022021/main/Smart%20segmentation%20values%20DATA%20240321.csv')
dataGit <- read_csv(url(URL))
```


# Logistic regression for a single variable - preparing the data set
## Testing the data set and removing outlaiers + transforming the dependent variables for integers 
```{r, warning=FALSE, message=FALSE}
skim(dataGit)
dataGit <- dataGit %>% 
  mutate(spin_habit_group = as.numeric(spin_habit_group))

continuousOutliers <- function(data, lowerBound = 3, upperBound = 3){
  
  # This function takes a vector, and replace observations that are smaller or bigger than the lower/upper bounds * standard deviations with NA
  
  Mean   <- mean(data, na.rm = TRUE)
  SD     <- sd(data, na.rm = TRUE)
  High   <- Mean + upperBound * SD # Upper Range  
  Low    <- Mean - lowerBound * SD # Lower Range
  Final  <- ifelse(data > High | data < Low, NA, data)
  
  return(Final)
  
}


Variables <- c('ltv', 'spin_habit_group', 'weekly_play_days', 'bet_habit_group', 'daily_wager_group', 'cz_group')

cleanData <- dataGit

for(varName in Variables){
  cleanData[, varName] <- continuousOutliers(unlist(cleanData[, varName]))
}

newData <- cleanData %>% 
    select(ltv, spin_habit_group, weekly_play_days, bet_habit_group, daily_wager_group, cz_group) %>% 
    drop_na() %>% 
  mutate(DV = ifelse(ltv == 0, 0, 1)) 
```

Using the median we can decide how how to split weekly play days into high and low subgroups (this is only for example, usually you will prefer to use the continuous variables).
However, it is highly recommended to use theory (prior information) for splitting the data set, and not only using the distribution.

```{r, warning=FALSE, message=FALSE}
hist(newData$weekly_play_days)
weekly_play_days_median <- median(newData$weekly_play_days)
weekly_play_days_median

newData <- newData %>% 
  mutate(weekly_play_days = ifelse(weekly_play_days <= weekly_play_days_median, 'Low', 'High'),
         weekly_play_days = as.factor(weekly_play_days))
```


## Performing the model
Using the **glm** command we can perform a logistic regression
```{r, warning=FALSE, message=FALSE}
regLog1 <- glm(DV ~ spin_habit_group + weekly_play_days + bet_habit_group + daily_wager_group + cz_group, family = binomial('logit'), data = newData)

summary(regLog1)
```

## Odss ratio
Extracting the odds ratio
```{r, warning=FALSE, message=FALSE}
ORCI <- round(exp(cbind(coef(regLog1), confint(regLog1))), 2) 
ORCI
```

## Testing the model's significancy
### Chi-square value

```{r, warning=FALSE, message=FALSE}
cdiff  <- regLog1$null.deviance - regLog1$deviance
round(cdiff, 2)
```

### Degress of freedom

```{r, warning=FALSE, message=FALSE}
dfdiff <- regLog1$df.null - regLog1$df.residual
round(dfdiff, 2)
```

Using the **pchisq** command we can find the model's significance.
### p value
```{r, warning=FALSE, message=FALSE}
round(pchisq(cdiff, dfdiff, lower.tail = FALSE), 2)
```

### Explained variance

Using the **nagelkerke** command from the **rcompanion** package we can find the explained variance.
```{r, warning=FALSE, message=FALSE}
Nagelkerke <- nagelkerke(regLog1, null = NULL, restrictNobs = FALSE) # rcompanion package
paste0(100 * round(Nagelkerke$Pseudo.R.squared.for.model.vs.null[3], 4), "%")
```

### Fit of the model - Holsem test
Using the **hoslem.test** command from the **ResourceSelection** package we can find the model's fit.
```{r, warning=FALSE, message=FALSE}
hoslem.test(regLog1$y, fitted(regLog1), g = 10) # ResourceSelection package
```

# Model accuracy
```{r, warning=FALSE, message=FALSE}
Correct <- regLog1$fitted.values
binaryCorrect <- as.numeric(ifelse(Correct > 0.5, 1, 0))
Prediction <- table(newData$DV, binaryCorrect)
```

## Confusion matrix
```{r, warning=FALSE, message=FALSE}
Prediction
```

## Percent accuracy 
```{r, warning=FALSE, message=FALSE}
paste0(round((Prediction[1, 1] + Prediction[2, 2]) / sum(Prediction[, ]) * 100, 2), "%")
```



